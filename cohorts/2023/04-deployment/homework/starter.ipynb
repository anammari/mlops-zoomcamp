{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c51efaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn                  1.2.1\n",
      "scikit-learn-intelex          20230228.214242\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ef880a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7836ccfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.10/site-packages/sklearn/base.py:299: UserWarning: Trying to unpickle estimator DictVectorizer from version 1.2.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/lib/python3.10/site-packages/sklearn/base.py:299: UserWarning: Trying to unpickle estimator LinearRegression from version 1.2.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with open('model.bin', 'rb') as f_in:\n",
    "    dv, model = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41c08294",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['PULocationID', 'DOLocationID']\n",
    "\n",
    "def read_data(filename):\n",
    "    df = pd.read_parquet(filename)\n",
    "    \n",
    "    df['duration'] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime\n",
    "    df['duration'] = df.duration.dt.total_seconds() / 60\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)].copy()\n",
    "\n",
    "    df[categorical] = df[categorical].fillna(-1).astype('int').astype('str')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4854399a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_data('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-02.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "669fda0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts = df[categorical].to_dict(orient='records')\n",
    "X_val = dv.transform(dicts)\n",
    "y_pred = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ec6c356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.281404481465004"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['y_pred'] = y_pred\n",
    "df['y_pred'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21c6b691",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2022\n",
    "month = 2\n",
    "output_file = 'output.parquet'\n",
    "df['ride_id'] = f'{year:04d}/{month:02d}_' + df.index.astype('str')\n",
    "df_result = df [['ride_id', 'y_pred']]\n",
    "df_result.to_parquet(\n",
    "    output_file,\n",
    "    engine='pyarrow',\n",
    "    compression=None,\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be6f78e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the parquet file is 57.215529441833496 MB.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_parquet_file_size_in_mb(parquet_file_path):\n",
    "  \"\"\"Returns the size of the parquet file in megabytes.\n",
    "\n",
    "  Args:\n",
    "    parquet_file_path: The path to the parquet file.\n",
    "\n",
    "  Returns:\n",
    "    The size of the parquet file in megabytes.\n",
    "  \"\"\"\n",
    "\n",
    "  file_size_bytes = os.stat(parquet_file_path).st_size\n",
    "  file_size_mb = file_size_bytes / (1024 * 1024)\n",
    "  return file_size_mb\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  parquet_file_path = output_file\n",
    "  file_size_mb = get_parquet_file_size_in_mb(parquet_file_path)\n",
    "  print(f\"The size of the parquet file is {file_size_mb} MB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec149b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook starter.ipynb to python\n",
      "[NbConvertApp] Writing 1869 bytes to starter.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to python starter.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0c9bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
